{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T13:53:40.056060Z",
     "start_time": "2025-11-03T13:48:24.259166Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mamy0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 80000/80000 [08:30<00:00, 156.66it/s]\n",
      "100%|██████████| 80000/80000 [08:22<00:00, 159.11it/s]\n",
      "100%|██████████| 16000/16000 [01:31<00:00, 174.25it/s]\n",
      "100%|██████████| 16000/16000 [01:32<00:00, 173.35it/s]\n",
      "100%|██████████| 80000/80000 [13:08<00:00, 101.40it/s]\n",
      "100%|██████████| 80000/80000 [13:09<00:00, 101.34it/s]\n",
      "100%|██████████| 16000/16000 [02:03<00:00, 129.08it/s]\n",
      "100%|██████████| 16000/16000 [02:06<00:00, 126.00it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "train_dataset = load_dataset(\"swan07/authorship-verification\", split=\"train[:80000]\")\n",
    "test_dataset = load_dataset(\"swan07/authorship-verification\", split=\"test[:16000]\")\n",
    "\n",
    "from specter import SpecterVectorizer\n",
    "\n",
    "spectorVectorizer = SpecterVectorizer()\n",
    "spectorVectorizer.load()\n",
    "\n",
    "train_spector = spectorVectorizer.embed(train_dataset)\n",
    "test_spector = spectorVectorizer.embed(test_dataset)\n",
    "\n",
    "from bert import BertVectorizer\n",
    "\n",
    "bertVectorizer = BertVectorizer()\n",
    "bertVectorizer.load()\n",
    "\n",
    "train_bert = bertVectorizer.embed(train_dataset)\n",
    "test_bert = bertVectorizer.embed(test_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "916b4206e44ee798",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 중...\n",
      "✓ train_specter.pkl\n",
      "✓ test_specter.pkl\n",
      "✓ train_bert.pkl\n",
      "✓ test_bert.pkl\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "Path(\"./embeddings\").mkdir(exist_ok=True)\n",
    "\n",
    "# 저장\n",
    "print(\"저장 중...\")\n",
    "with open('./embeddings/train_specter.pkl', 'wb') as f:\n",
    "    pickle.dump(train_spector, f)\n",
    "print(\"✓ train_specter.pkl\")\n",
    "\n",
    "with open('./embeddings/test_specter.pkl', 'wb') as f:\n",
    "    pickle.dump(test_spector, f)\n",
    "print(\"✓ test_specter.pkl\")\n",
    "\n",
    "with open('./embeddings/train_bert.pkl', 'wb') as f:\n",
    "    pickle.dump(train_bert, f)\n",
    "print(\"✓ train_bert.pkl\")\n",
    "\n",
    "with open('./embeddings/test_bert.pkl', 'wb') as f:\n",
    "    pickle.dump(test_bert, f)\n",
    "print(\"✓ test_bert.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c0ac9c7170173865",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T16:05:45.074191Z",
     "start_time": "2025-12-06T16:05:42.989853Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "with open('./embeddings/train_bert.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open('./embeddings/test_bert.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "ea1cced09fb7b5f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T16:09:20.512861Z",
     "start_time": "2025-12-06T16:09:19.823065Z"
    }
   },
   "source": [
    "from classifier import train_authorship_classifiers\n",
    "models, results = train_authorship_classifiers(train_data, test_data, True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n",
      "Preparing features on GPU…\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mclassifier\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m train_authorship_classifiers\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m models, results = \u001B[43mtrain_authorship_classifiers\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\COSE36203\\src\\classifier.py:154\u001B[39m, in \u001B[36mtrain_authorship_classifiers\u001B[39m\u001B[34m(train_df, test_df, use_gpu)\u001B[39m\n\u001B[32m    151\u001B[39m y_test = test_df[\u001B[33m'\u001B[39m\u001B[33msame\u001B[39m\u001B[33m'\u001B[39m].values\n\u001B[32m    153\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mPreparing features on GPU…\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m154\u001B[39m train_features = \u001B[43mprepare_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    155\u001B[39m test_features = prepare_features(test_df, device)\n\u001B[32m    157\u001B[39m X_train = train_features[\u001B[33m'\u001B[39m\u001B[33mcombined\u001B[39m\u001B[33m'\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\COSE36203\\src\\classifier.py:104\u001B[39m, in \u001B[36mprepare_features\u001B[39m\u001B[34m(df, device)\u001B[39m\n\u001B[32m    103\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mprepare_features\u001B[39m(df, device=\u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m104\u001B[39m     vec1 = \u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mtext1_vec\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    105\u001B[39m     vec2 = np.stack(df[\u001B[33m'\u001B[39m\u001B[33mtext2_vec\u001B[39m\u001B[33m'\u001B[39m].values)\n\u001B[32m    107\u001B[39m     t1 = torch.tensor(vec1, dtype=torch.float32, device=device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LILAB\\.venv\\Lib\\site-packages\\numpy\\_core\\shape_base.py:463\u001B[39m, in \u001B[36mstack\u001B[39m\u001B[34m(arrays, axis, out, dtype, casting)\u001B[39m\n\u001B[32m    460\u001B[39m axis = normalize_axis_index(axis, result_ndim)\n\u001B[32m    462\u001B[39m sl = (\u001B[38;5;28mslice\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m),) * axis + (_nx.newaxis,)\n\u001B[32m--> \u001B[39m\u001B[32m463\u001B[39m expanded_arrays = [arr[sl] \u001B[38;5;28;01mfor\u001B[39;00m arr \u001B[38;5;129;01min\u001B[39;00m arrays]\n\u001B[32m    464\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m _nx.concatenate(expanded_arrays, axis=axis, out=out,\n\u001B[32m    465\u001B[39m                        dtype=dtype, casting=casting)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3847526",
   "metadata": {},
   "outputs": [],
   "source": "results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fad2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier import MajorityVotingEnsemble\n",
    "import pandas as pd\n",
    "\n",
    "majority_voting = MajorityVotingEnsemble(models.keys())\n",
    "df_pred = pd.DataFrame({\n",
    "    \"true_label\": test_data[\"same\"],  # 정답 레이블\n",
    "    \"pred_lightgbm\": results.iloc[0][\"pred_test\"],\n",
    "    \"pred_xgboost\": results.iloc[1][\"pred_test\"],\n",
    "    \"pred_catboost\": results.iloc[2][\"pred_test\"],\n",
    "    \"pred_simple_nn\": results.iloc[3][\"pred_test\"],\n",
    "})\n",
    "majority_voting.evaluate(df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f26b017731db3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
