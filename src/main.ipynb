{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T13:53:40.056060Z",
     "start_time": "2025-11-03T13:48:24.259166Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mamy0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80000/80000 [08:30<00:00, 156.66it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80000/80000 [08:22<00:00, 159.11it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16000/16000 [01:31<00:00, 174.25it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16000/16000 [01:32<00:00, 173.35it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80000/80000 [13:08<00:00, 101.40it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80000/80000 [13:09<00:00, 101.34it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16000/16000 [02:03<00:00, 129.08it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16000/16000 [02:06<00:00, 126.00it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "train_dataset = load_dataset(\"swan07/authorship-verification\", split=\"train[:80000]\")\n",
    "test_dataset = load_dataset(\"swan07/authorship-verification\", split=\"test[:16000]\")\n",
    "\n",
    "from specter import SpecterVectorizer\n",
    "\n",
    "spectorVectorizer = SpecterVectorizer()\n",
    "spectorVectorizer.load()\n",
    "\n",
    "train_spector = spectorVectorizer.embed(train_dataset)\n",
    "test_spector = spectorVectorizer.embed(test_dataset)\n",
    "\n",
    "from bert import BertVectorizer\n",
    "\n",
    "bertVectorizer = BertVectorizer()\n",
    "bertVectorizer.load()\n",
    "\n",
    "train_bert = bertVectorizer.embed(train_dataset)\n",
    "test_bert = bertVectorizer.embed(test_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "916b4206e44ee798",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì €ì¥ ì¤‘...\n",
      "âœ“ train_specter.pkl\n",
      "âœ“ test_specter.pkl\n",
      "âœ“ train_bert.pkl\n",
      "âœ“ test_bert.pkl\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "Path(\"./embeddings\").mkdir(exist_ok=True)\n",
    "\n",
    "# ì €ì¥\n",
    "print(\"ì €ì¥ ì¤‘...\")\n",
    "with open('./embeddings/train_specter.pkl', 'wb') as f:\n",
    "    pickle.dump(train_spector, f)\n",
    "print(\"âœ“ train_specter.pkl\")\n",
    "\n",
    "with open('./embeddings/test_specter.pkl', 'wb') as f:\n",
    "    pickle.dump(test_spector, f)\n",
    "print(\"âœ“ test_specter.pkl\")\n",
    "\n",
    "with open('./embeddings/train_bert.pkl', 'wb') as f:\n",
    "    pickle.dump(train_bert, f)\n",
    "print(\"âœ“ train_bert.pkl\")\n",
    "\n",
    "with open('./embeddings/test_bert.pkl', 'wb') as f:\n",
    "    pickle.dump(test_bert, f)\n",
    "print(\"âœ“ test_bert.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0ac9c7170173865",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T14:37:33.409471Z",
     "start_time": "2025-11-03T14:37:33.039698Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "\n",
    "with open('./embeddings/train_specter.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open('./embeddings/test_specter.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "# with open('./embeddings/train_bert.pkl', 'rb') as f:\n",
    "#     train_data = pickle.load(f)\n",
    "# with open('./embeddings/test_bert.pkl', 'rb') as f:\n",
    "#     test_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1cced09fb7b5f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T14:43:13.575262Z",
     "start_time": "2025-11-03T14:37:33.418265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ì €ì ë™ì¼ì„± ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (Selected 3 Models)\n",
      "============================================================\n",
      "\n",
      "í•™ìŠµ ë°ì´í„° í¬ê¸°: 80000 samples\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: 16000 samples\n",
      "\n",
      "[Train] í´ë˜ìŠ¤ ë¶„í¬ - ê°™ì€ ì €ì: 45491, ë‹¤ë¥¸ ì €ì: 34509\n",
      "[Test] í´ë˜ìŠ¤ ë¶„í¬ - ê°™ì€ ì €ì: 8265, ë‹¤ë¥¸ ì €ì: 7735\n",
      "\n",
      "Feature ì¤€ë¹„ ì¤‘...\n",
      "\n",
      "ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda\n",
      "\n",
      "============================================================\n",
      "1. SVM ë¶„ë¥˜ê¸° í•™ìŠµ\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from classifier import train_authorship_classifiers\n",
    "models, results, train_preds_df, test_preds_df = train_authorship_classifiers(train_data, test_data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3847526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import result_saver as rs\n",
    "rs.save_results(models, results, train_preds_df, test_preds_df, './results/specter')\n",
    "#rs.save_results(models, results, train_preds_df, test_preds_df, './results/bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fad2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "models, results, train_preds_df, test_preds_df = rs.load_results('./results/specter')\n",
    "#models, results, train_preds_df, test_preds_df = rs.load_results('./results/bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362148825e772eeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T14:50:01.208243Z",
     "start_time": "2025-11-03T14:50:01.003646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš© ì˜ˆì‹œ:\n",
      "------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ“Š ëª¨ë¸ í‰ê°€ ê²°ê³¼ (1,000 samples)\n",
      "====================================================================================================\n",
      "\n",
      "Model                  Accuracy  Precision     Recall   F1-Score        MCC\n",
      "----------------------------------------------------------------------------------------------------\n",
      "svm                      0.6380     0.6519     0.5671     0.6065     0.2766\n",
      "simple_nn                0.6360     0.6356     0.6098     0.6224     0.2716\n",
      "nb                       0.6270     0.6112     0.6646     0.6368     0.2558\n",
      "deep_nn                  0.6180     0.6083     0.6280     0.6180     0.2363\n",
      "cosine                   0.6170     0.6598     0.4573     0.5402     0.2415\n",
      "rf                       0.6170     0.6589     0.4593     0.5413     0.2412\n",
      "siamese_nn               0.5820     0.5654     0.6504     0.6049     0.1676\n",
      "lr                       0.5740     0.5668     0.5691     0.5680     0.1478\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: svm\n",
      "   Accuracy: 0.6380\n",
      "   F1-Score: 0.6065\n",
      "\n",
      "ğŸ“‹ ìƒìœ„ 3ê°œ ëª¨ë¸ ìƒì„¸ ì •ë³´:\n",
      "====================================================================================================\n",
      "\n",
      "1. COSINE\n",
      "   Accuracy: 0.6170 | Precision: 0.6598 | Recall: 0.4573 | F1: 0.5402\n",
      "\n",
      "   Confusion Matrix:\n",
      "              ì˜ˆì¸¡\n",
      "           0     1\n",
      "   ì‹¤ì œ 0   392   116\n",
      "        1   267   225\n",
      "\n",
      "2. SVM\n",
      "   Accuracy: 0.6380 | Precision: 0.6519 | Recall: 0.5671 | F1: 0.6065\n",
      "\n",
      "   Confusion Matrix:\n",
      "              ì˜ˆì¸¡\n",
      "           0     1\n",
      "   ì‹¤ì œ 0   359   149\n",
      "        1   213   279\n",
      "\n",
      "3. RF\n",
      "   Accuracy: 0.6170 | Precision: 0.6589 | Recall: 0.4593 | F1: 0.5413\n",
      "\n",
      "   Confusion Matrix:\n",
      "              ì˜ˆì¸¡\n",
      "           0     1\n",
      "   ì‹¤ì œ 0   391   117\n",
      "        1   266   226\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "F1-Score ê¸°ì¤€ ëª¨ë¸ ë¹„êµ:\n",
      "        model  f1_score  accuracy\n",
      "0          nb  0.636806     0.627\n",
      "1   simple_nn  0.622407     0.636\n",
      "2     deep_nn  0.618000     0.618\n",
      "3         svm  0.606522     0.638\n",
      "4  siamese_nn  0.604915     0.582\n",
      "5          lr  0.567951     0.574\n",
      "6          rf  0.541317     0.617\n",
      "7      cosine  0.540216     0.617\n",
      "\n",
      "ì•™ìƒë¸” ì •í™•ë„: 0.6450\n",
      "\n",
      "============================================================\n",
      "ğŸ” ëª¨ë¸ ê°„ ì˜ê²¬ ë¶ˆì¼ì¹˜ ë¶„ì„\n",
      "============================================================\n",
      "ì „ì²´ ìƒ˜í”Œ: 1,000\n",
      "ì „ì› ì¼ì¹˜: 229 (22.9%)\n",
      "ì˜ê²¬ ë¶ˆì¼ì¹˜: 771 (77.1%)\n",
      "ì˜ê²¬ ë¶ˆì¼ì¹˜ ìƒ˜í”Œì˜ ì•™ìƒë¸” ì •í™•ë„: 0.6122\n",
      "============================================================\n",
      "\n",
      "ì˜ê²¬ ë¶ˆì¼ì¹˜ ìƒ˜í”Œ ìˆ˜: 771\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "class MultiModelEvaluator:\n",
    "    \"\"\"ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‰ê°€í•˜ëŠ” í´ë˜ìŠ¤\"\"\"\n",
    "\n",
    "    def __init__(self, pred_columns: List[str] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred_columns: í‰ê°€í•  ì˜ˆì¸¡ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ìë™ ê°ì§€)\n",
    "        \"\"\"\n",
    "        self.pred_columns = pred_columns\n",
    "\n",
    "    def _get_pred_columns(self, df: pd.DataFrame) -> List[str]:\n",
    "        \"\"\"ì˜ˆì¸¡ ì»¬ëŸ¼ ìë™ ê°ì§€\"\"\"\n",
    "        if self.pred_columns is not None:\n",
    "            return self.pred_columns\n",
    "\n",
    "        # 'pred_'ë¡œ ì‹œì‘í•˜ëŠ” ì»¬ëŸ¼ ì°¾ê¸°\n",
    "        pred_cols = [col for col in df.columns if col.startswith('pred_')]\n",
    "        if not pred_cols:\n",
    "            raise ValueError(\"ì˜ˆì¸¡ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'pred_'ë¡œ ì‹œì‘í•˜ëŠ” ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "        return pred_cols\n",
    "\n",
    "    def evaluate_single_model(self, y_true: np.ndarray, y_pred: np.ndarray,\n",
    "                            model_name: str = \"\") -> Dict:\n",
    "        \"\"\"\n",
    "        ë‹¨ì¼ ëª¨ë¸ í‰ê°€\n",
    "\n",
    "        Args:\n",
    "            y_true: ì‹¤ì œ ë ˆì´ë¸”\n",
    "            y_pred: ì˜ˆì¸¡ê°’\n",
    "            model_name: ëª¨ë¸ ì´ë¦„\n",
    "\n",
    "        Returns:\n",
    "            í‰ê°€ ì§€í‘œ ë”•ì…”ë„ˆë¦¬\n",
    "        \"\"\"\n",
    "        metrics = {\n",
    "            'model': model_name,\n",
    "            'n_samples': len(y_true),\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "            'f1_score': f1_score(y_true, y_pred, zero_division=0),\n",
    "            'mcc': matthews_corrcoef(y_true, y_pred)\n",
    "        }\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        metrics['confusion_matrix'] = {\n",
    "            'tn': int(cm[0, 0]),\n",
    "            'fp': int(cm[0, 1]),\n",
    "            'fn': int(cm[1, 0]),\n",
    "            'tp': int(cm[1, 1])\n",
    "        }\n",
    "\n",
    "        # Specificityì™€ ê¸°íƒ€ ì§€í‘œ\n",
    "        tn, fp, fn, tp = cm[0, 0], cm[0, 1], cm[1, 0], cm[1, 1]\n",
    "        metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        metrics['sensitivity'] = tp / (tp + fn) if (tp + fn) > 0 else 0  # = recall\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def evaluate_all_models(self, df: pd.DataFrame,\n",
    "                          true_label_col: str = 'true_label') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        ëª¨ë“  ëª¨ë¸ í‰ê°€\n",
    "\n",
    "        Args:\n",
    "            df: ì˜ˆì¸¡ ê²°ê³¼ê°€ í¬í•¨ëœ ë°ì´í„°í”„ë ˆì„\n",
    "            true_label_col: ì‹¤ì œ ë ˆì´ë¸” ì»¬ëŸ¼ëª…\n",
    "\n",
    "        Returns:\n",
    "            ëª¨ë“  ëª¨ë¸ì˜ í‰ê°€ ì§€í‘œë¥¼ ë‹´ì€ DataFrame\n",
    "        \"\"\"\n",
    "        if true_label_col not in df.columns:\n",
    "            raise ValueError(f\"'{true_label_col}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "        pred_cols = self._get_pred_columns(df)\n",
    "        y_true = df[true_label_col].values\n",
    "\n",
    "        results = []\n",
    "        for pred_col in pred_cols:\n",
    "            y_pred = df[pred_col].values\n",
    "            model_name = pred_col.replace('pred_', '')\n",
    "\n",
    "            metrics = self.evaluate_single_model(y_true, y_pred, model_name)\n",
    "            results.append(metrics)\n",
    "\n",
    "        # Confusion matrix ì œì™¸í•˜ê³  DataFrame ìƒì„±\n",
    "        results_simple = []\n",
    "        for r in results:\n",
    "            simple = {k: v for k, v in r.items() if k != 'confusion_matrix'}\n",
    "            results_simple.append(simple)\n",
    "\n",
    "        results_df = pd.DataFrame(results_simple)\n",
    "\n",
    "        # ì •ë ¬ (accuracy ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ)\n",
    "        results_df = results_df.sort_values('accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        return results_df, results\n",
    "\n",
    "    def print_report(self, df: pd.DataFrame, true_label_col: str = 'true_label'):\n",
    "        \"\"\"\n",
    "        í‰ê°€ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥\n",
    "\n",
    "        Args:\n",
    "            df: í‰ê°€í•  ë°ì´í„°í”„ë ˆì„\n",
    "            true_label_col: ì‹¤ì œ ë ˆì´ë¸” ì»¬ëŸ¼ëª…\n",
    "        \"\"\"\n",
    "        results_df, full_results = self.evaluate_all_models(df, true_label_col)\n",
    "\n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"ğŸ“Š ëª¨ë¸ í‰ê°€ ê²°ê³¼ ({len(df):,} samples)\")\n",
    "        print(f\"{'='*100}\")\n",
    "\n",
    "        # ë©”ì¸ í…Œì´ë¸”\n",
    "        print(f\"\\n{'Model':<20} {'Accuracy':>10} {'Precision':>10} {'Recall':>10} \"\n",
    "              f\"{'F1-Score':>10} {'MCC':>10}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "        for _, row in results_df.iterrows():\n",
    "            print(f\"{row['model']:<20} {row['accuracy']:>10.4f} {row['precision']:>10.4f} \"\n",
    "                  f\"{row['recall']:>10.4f} {row['f1_score']:>10.4f} {row['mcc']:>10.4f}\")\n",
    "\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸\n",
    "        best_model = results_df.iloc[0]\n",
    "        print(f\"\\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['model']}\")\n",
    "        print(f\"   Accuracy: {best_model['accuracy']:.4f}\")\n",
    "        print(f\"   F1-Score: {best_model['f1_score']:.4f}\")\n",
    "\n",
    "        # ìƒì„¸ Confusion Matrix (ìƒìœ„ 3ê°œ ëª¨ë¸)\n",
    "        print(f\"\\nğŸ“‹ ìƒìœ„ 3ê°œ ëª¨ë¸ ìƒì„¸ ì •ë³´:\")\n",
    "        print(\"=\" * 100)\n",
    "\n",
    "        for i in range(min(3, len(full_results))):\n",
    "            result = full_results[i]\n",
    "            cm = result['confusion_matrix']\n",
    "\n",
    "            print(f\"\\n{i+1}. {result['model'].upper()}\")\n",
    "            print(f\"   Accuracy: {result['accuracy']:.4f} | Precision: {result['precision']:.4f} | \"\n",
    "                  f\"Recall: {result['recall']:.4f} | F1: {result['f1_score']:.4f}\")\n",
    "            print(f\"\\n   Confusion Matrix:\")\n",
    "            print(f\"              ì˜ˆì¸¡\")\n",
    "            print(f\"           0     1\")\n",
    "            print(f\"   ì‹¤ì œ 0  {cm['tn']:4d}  {cm['fp']:4d}\")\n",
    "            print(f\"        1  {cm['fn']:4d}  {cm['tp']:4d}\")\n",
    "\n",
    "        print(f\"\\n{'='*100}\\n\")\n",
    "\n",
    "    def compare_models(self, df: pd.DataFrame, true_label_col: str = 'true_label',\n",
    "                      metric: str = 'accuracy') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        íŠ¹ì • ì§€í‘œ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë¸ ë¹„êµ\n",
    "\n",
    "        Args:\n",
    "            df: ì˜ˆì¸¡ ê²°ê³¼ DataFrame\n",
    "            true_label_col: ì‹¤ì œ ë ˆì´ë¸” ì»¬ëŸ¼ëª…\n",
    "            metric: ë¹„êµí•  ì§€í‘œ ('accuracy', 'f1_score', 'precision', 'recall', 'mcc')\n",
    "\n",
    "        Returns:\n",
    "            ì§€í‘œ ê¸°ì¤€ ì •ë ¬ëœ DataFrame\n",
    "        \"\"\"\n",
    "        results_df, _ = self.evaluate_all_models(df, true_label_col)\n",
    "\n",
    "        if metric not in results_df.columns:\n",
    "            raise ValueError(f\"'{metric}' ì§€í‘œê°€ ì—†ìŠµë‹ˆë‹¤. \"\n",
    "                           f\"ê°€ëŠ¥í•œ ì§€í‘œ: {results_df.columns.tolist()}\")\n",
    "\n",
    "        return results_df.sort_values(metric, ascending=False).reset_index(drop=True)\n",
    "\n",
    "    def get_ensemble_prediction(self, df: pd.DataFrame, method: str = 'majority') -> pd.Series:\n",
    "        \"\"\"\n",
    "        ì•™ìƒë¸” ì˜ˆì¸¡ ìƒì„±\n",
    "\n",
    "        Args:\n",
    "            df: ì˜ˆì¸¡ ê²°ê³¼ DataFrame\n",
    "            method: 'majority' (ë‹¤ìˆ˜ê²°) ë˜ëŠ” 'unanimous' (ë§Œì¥ì¼ì¹˜)\n",
    "\n",
    "        Returns:\n",
    "            ì•™ìƒë¸” ì˜ˆì¸¡ Series\n",
    "        \"\"\"\n",
    "        pred_cols = self._get_pred_columns(df)\n",
    "\n",
    "        if method == 'majority':\n",
    "            # ë‹¤ìˆ˜ê²° íˆ¬í‘œ\n",
    "            ensemble = df[pred_cols].mode(axis=1)[0]\n",
    "        elif method == 'unanimous':\n",
    "            # ëª¨ë“  ëª¨ë¸ì´ ë™ì˜í•˜ëŠ” ê²½ìš°ë§Œ 1\n",
    "            ensemble = df[pred_cols].all(axis=1).astype(int)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "        return ensemble\n",
    "\n",
    "    def analyze_disagreements(self, df: pd.DataFrame,\n",
    "                             true_label_col: str = 'true_label') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        ëª¨ë¸ ê°„ ì˜ê²¬ ë¶ˆì¼ì¹˜ ë¶„ì„\n",
    "\n",
    "        Args:\n",
    "            df: ì˜ˆì¸¡ ê²°ê³¼ DataFrame\n",
    "            true_label_col: ì‹¤ì œ ë ˆì´ë¸” ì»¬ëŸ¼ëª…\n",
    "\n",
    "        Returns:\n",
    "            ì˜ê²¬ ë¶ˆì¼ì¹˜ í†µê³„ DataFrame\n",
    "        \"\"\"\n",
    "        pred_cols = self._get_pred_columns(df)\n",
    "\n",
    "        # ê° ìƒ˜í”Œì— ëŒ€í•´ ì˜ˆì¸¡ì´ ì¼ì¹˜í•˜ì§€ ì•Šì€ ì •ë„ ê³„ì‚°\n",
    "        df_copy = df.copy()\n",
    "        df_copy['n_agree'] = df_copy[pred_cols].sum(axis=1)  # 1ë¡œ ì˜ˆì¸¡í•œ ëª¨ë¸ ìˆ˜\n",
    "        df_copy['all_agree'] = (df_copy['n_agree'] == 0) | (df_copy['n_agree'] == len(pred_cols))\n",
    "        df_copy['disagreement'] = ~df_copy['all_agree']\n",
    "\n",
    "        # í†µê³„\n",
    "        stats = {\n",
    "            'total_samples': len(df),\n",
    "            'all_agree': df_copy['all_agree'].sum(),\n",
    "            'disagreement': df_copy['disagreement'].sum(),\n",
    "            'disagreement_rate': df_copy['disagreement'].mean()\n",
    "        }\n",
    "\n",
    "        # ì˜ê²¬ ë¶ˆì¼ì¹˜ ìƒ˜í”Œ ì¤‘ ì •ë‹µë¥ \n",
    "        if stats['disagreement'] > 0:\n",
    "            disagreement_samples = df_copy[df_copy['disagreement']]\n",
    "            ensemble_pred = self.get_ensemble_prediction(disagreement_samples, method='majority')\n",
    "            stats['disagreement_accuracy'] = accuracy_score(\n",
    "                disagreement_samples[true_label_col],\n",
    "                ensemble_pred\n",
    "            )\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ” ëª¨ë¸ ê°„ ì˜ê²¬ ë¶ˆì¼ì¹˜ ë¶„ì„\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"ì „ì²´ ìƒ˜í”Œ: {stats['total_samples']:,}\")\n",
    "        print(f\"ì „ì› ì¼ì¹˜: {stats['all_agree']:,} ({stats['all_agree']/stats['total_samples']*100:.1f}%)\")\n",
    "        print(f\"ì˜ê²¬ ë¶ˆì¼ì¹˜: {stats['disagreement']:,} ({stats['disagreement_rate']*100:.1f}%)\")\n",
    "\n",
    "        if 'disagreement_accuracy' in stats:\n",
    "            print(f\"ì˜ê²¬ ë¶ˆì¼ì¹˜ ìƒ˜í”Œì˜ ì•™ìƒë¸” ì •í™•ë„: {stats['disagreement_accuracy']:.4f}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        return df_copy[df_copy['disagreement']]\n",
    "\n",
    "\n",
    "def evaluate_predictions(train_preds_df: pd.DataFrame,\n",
    "                        test_preds_df: pd.DataFrame,\n",
    "                        true_label_col: str = 'true_label',\n",
    "                        verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Train/Test ì˜ˆì¸¡ ê²°ê³¼ í‰ê°€ ë©”ì¸ í•¨ìˆ˜\n",
    "\n",
    "    Args:\n",
    "        train_preds_df: Train ì˜ˆì¸¡ ê²°ê³¼ DataFrame\n",
    "        test_preds_df: Test ì˜ˆì¸¡ ê²°ê³¼ DataFrame\n",
    "        true_label_col: ì‹¤ì œ ë ˆì´ë¸” ì»¬ëŸ¼ëª…\n",
    "        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€\n",
    "\n",
    "    Returns:\n",
    "        train_results, test_results (ê°ê° í‰ê°€ ê²°ê³¼ DataFrame)\n",
    "    \"\"\"\n",
    "    evaluator = MultiModelEvaluator()\n",
    "\n",
    "    # Train í‰ê°€\n",
    "    print(\"\\n\" + \"ğŸ“ TRAIN SET í‰ê°€ \" + \"=\"*80)\n",
    "    if verbose:\n",
    "        evaluator.print_report(train_preds_df, true_label_col)\n",
    "    train_results, _ = evaluator.evaluate_all_models(train_preds_df, true_label_col)\n",
    "\n",
    "    # Test í‰ê°€\n",
    "    print(\"\\n\" + \"ğŸ§ª TEST SET í‰ê°€ \" + \"=\"*80)\n",
    "    if verbose:\n",
    "        evaluator.print_report(test_preds_df, true_label_col)\n",
    "    test_results, _ = evaluator.evaluate_all_models(test_preds_df, true_label_col)\n",
    "\n",
    "    # ë¹„êµ\n",
    "    print(\"\\n\" + \"ğŸ“Š TRAIN vs TEST ë¹„êµ \" + \"=\"*80)\n",
    "    comparison = pd.merge(\n",
    "        train_results[['model', 'accuracy', 'f1_score']],\n",
    "        test_results[['model', 'accuracy', 'f1_score']],\n",
    "        on='model',\n",
    "        suffixes=('_train', '_test')\n",
    "    )\n",
    "    comparison['overfitting'] = comparison['accuracy_train'] - comparison['accuracy_test']\n",
    "    comparison = comparison.sort_values('accuracy_test', ascending=False)\n",
    "\n",
    "    print(f\"\\n{'Model':<20} {'Train Acc':>10} {'Test Acc':>10} {'Overfit':>10} {'Test F1':>10}\")\n",
    "    print(\"-\" * 80)\n",
    "    for _, row in comparison.iterrows():\n",
    "        print(f\"{row['model']:<20} {row['accuracy_train']:>10.4f} {row['accuracy_test']:>10.4f} \"\n",
    "              f\"{row['overfitting']:>10.4f} {row['f1_score_test']:>10.4f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    return train_results, test_results, comparison\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluator = MultiModelEvaluator()\n",
    "    evaluator.print_report(test_preds_df)\n",
    "\n",
    "    # íŠ¹ì • ì§€í‘œë¡œ ë¹„êµ\n",
    "    print(\"\\nF1-Score ê¸°ì¤€ ëª¨ë¸ ë¹„êµ:\")\n",
    "    f1_comparison = evaluator.compare_models(test_preds_df, metric='f1_score')\n",
    "    print(f1_comparison[['model', 'f1_score', 'accuracy']])\n",
    "\n",
    "    # ì•™ìƒë¸” ì˜ˆì¸¡\n",
    "    ensemble = evaluator.get_ensemble_prediction(test_preds_df, method='majority')\n",
    "    print(f\"\\nì•™ìƒë¸” ì •í™•ë„: {accuracy_score(test_preds_df['true_label'], ensemble):.4f}\")\n",
    "\n",
    "    # ì˜ê²¬ ë¶ˆì¼ì¹˜ ë¶„ì„\n",
    "    disagreements = evaluator.analyze_disagreements(test_preds_df)\n",
    "    print(f\"ì˜ê²¬ ë¶ˆì¼ì¹˜ ìƒ˜í”Œ ìˆ˜: {len(disagreements)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b9836b7a3e41bd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T15:40:51.230644Z",
     "start_time": "2025-11-03T15:40:51.226150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MultiModelEvaluator at 0x1b31196aa80>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f26b017731db3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
